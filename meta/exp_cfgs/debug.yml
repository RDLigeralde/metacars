world:
  seed: 42
  map: levine # either single map or dir to be sampled from in domain randomization
  num_agents: 2
  num_envs:
    count: 1 # set -1 to use number of cpu cores
    type: dummy # subproc or dummy
  num_beams: 540 # number of lidar beams
  timestep: 0.01
  observation_config: 
    type: frenet_marl # need the nested dict for the observation config
  reset_config:
    type: rl_random_static
  control_input: ['speed', 'steering_angle']
  render_mode: null # set null to disable rendering: only for single env eval / debugging

rewards:
  normalize_observations: True
  normalize_rewards: True

  velocity_reward_scale: 0.0
  vel_action_change_penalty: 0.0
  steer_action_change_penalty: 0.0
  turn_speed_penalty: 0.0

  milestone: 0.1
  milestone_increment: 0.1
  milestone_reward: 5

  progress_weight: 1.0
  stagnation_penalty: -0.1
  stagnation_cutoff: 0.02

  max_crash_penalty: 1
  decay_interval: 100000
  crash_curriculum: 100000
  delta_u_curriculum: 100000
  v_ref_curriculum: 1000000

  position_weight: 1.0
  overtaking_reward: 25.0


car:
  # dr_param_ex:
  #  min: x
  #  max: y
  mu: 0.6
  C_Sf: 4.718
  C_Sr: 5.4562
  width: 0.31
  length: 0.58
  m: 3.74
  I: 0.04712
  h: 0.074
  v_min: -2.0
  v_max: 2.0
  a_max: 6.34
  s_min: -0.4189
  s_max: 0.4189
  v_switch: 7.319
  sv_min: -3.2
  sv_max: 3.2

ppo_params:
  init_path: null # if resuming a run
  recurrent: False
  device: cpu # actually faster on cpu, for some reason
  learning_rate: 0.0001
  n_steps: 2048
  batch_size: 64
  n_epochs: 8
  gamma: 0.99
  gae_lambda: 0.95
  clip_range: 0.15
  clip_range_vf: null
  normalize_advantage: True
  ent_coef: 0.0
  vf_coef: 0.5
  max_grad_norm: 0.5
  use_sde: False
  sde_sample_freq: -1

train_params:
  total_timesteps: 100
  save_interval: 100 # set large to only save last policy
  log_interval: 1
  reset_num_timesteps: False
  progress_bar: False

log:
  log_tensorboard: False
  project_name: null # set null to disable wandb
  run_name: null
